{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92be53f6",
   "metadata": {},
   "source": [
    "###### ==================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04c01bd",
   "metadata": {},
   "source": [
    "# HTTP Requesting & Dumping File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da89f65",
   "metadata": {},
   "source": [
    "###### ==================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145aae07",
   "metadata": {},
   "source": [
    "### Run Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07659dbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy-financial in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from numpy-financial) (1.23.5)\n"
     ]
    }
   ],
   "source": [
    "%run \"C:\\\\Users\\DELL\\\\Desktop\\\\Projects\\\\Wealth Management System\\\\Python\\\\General Files\\\\Libraries.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3f3c3ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy-financial in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from numpy-financial) (1.23.5)\n"
     ]
    }
   ],
   "source": [
    "%run \"C:\\\\Users\\DELL\\\\Desktop\\\\Projects\\\\Wealth Management System\\\\Python\\\\General Files\\\\SQL_Connection_File.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518c8abf",
   "metadata": {},
   "source": [
    "### Class: Request Bond Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b02c170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Request_Bond_Data:\n",
    "    def __init__(self, constructor_link, date):\n",
    "        \"\"\"\n",
    "        Initializes the Bond_Data class.\n",
    "\n",
    "        Args:\n",
    "            constructor_link (str): The base URL for constructing the download link.\n",
    "            date (str): The date for which data should be fetched.\n",
    "        \"\"\"\n",
    "        self.constructor_link = constructor_link\n",
    "        self.date = date\n",
    "        \n",
    "        self.url = constructor_link + date + '.zip'\n",
    "        self.csv_file_name_1 = 'icdm' + date + '.csv'\n",
    "        self.csv_file_name_2 = 'wdm' + date + '.csv'\n",
    "\n",
    "    def Fetching(self):\n",
    "        \"\"\"\n",
    "        Fetches and processes bond data.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the fetched data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            encoded_url = urllib.parse.quote(self.url, safe=':/')\n",
    "\n",
    "            # Send a GET request to download the zip file\n",
    "            response = requests.get(encoded_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                \n",
    "            if response.status_code == 200:\n",
    "                # Save the zip file locally\n",
    "                with open('downloaded_bond_data.zip', 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "\n",
    "                # Extract the contents of the zip file to a folder\n",
    "                with zipfile.ZipFile('downloaded_bond_data.zip', 'r') as zip_ref:\n",
    "                    zip_ref.extractall('extracted_bond_data')\n",
    "\n",
    "                # Specify the directory where the CSV files are located\n",
    "                folder_path = 'extracted_bond_data'\n",
    "                \n",
    "                # Check if both CSV files exist in the folder\n",
    "                if (self.csv_file_name_1 in os.listdir(folder_path)) and (self.csv_file_name_2 in os.listdir(folder_path)):\n",
    "                    # Create the full path to the CSV files\n",
    "                    csv_file_path_1 = os.path.join(folder_path, self.csv_file_name_1)\n",
    "                    csv_file_path_2 = os.path.join(folder_path, self.csv_file_name_2)\n",
    "\n",
    "                    # Read both CSV files into pandas DataFrames\n",
    "                    dataframe_1 = pd.read_csv(csv_file_path_1, on_bad_lines='skip')\n",
    "                    dataframe_2 = pd.read_csv(csv_file_path_2, on_bad_lines='skip')\n",
    "                    \n",
    "                    # Check if the DataFrames have more columns than expected (e.g., 12 columns)\n",
    "                    expected_columns_dataframe_1 = [\n",
    "                        'Security Code', 'Issuer Name', \n",
    "                        'Coupon (%)', 'LTP', \n",
    "                        'Weighted Average Price', \n",
    "                        'Weighted Average Yield', \n",
    "                        'Maturity Date', \n",
    "                        'Turnover (Rs. Lakh)', \n",
    "                        'ISIN No.', 'Face Value'\n",
    "                    ]\n",
    "                    \n",
    "                    expected_columns_dataframe_2 = [\n",
    "                        'Scrip Code',\n",
    "                        'Security Code',\n",
    "                        'Security Description',\n",
    "                        'Coupon Rate (%)',\n",
    "                        'Open Price',\n",
    "                        'High Price',\n",
    "                        'Low Price',\n",
    "                        'Close Price',\n",
    "                        'No. of Trades',\n",
    "                        'Total Trade Volume (Face Value in Rs. Lakh)',\n",
    "                        'Total Trade Turnover (Rs. Lakh)',\n",
    "                        'MaturityDate'\n",
    "                    ]\n",
    "                    \n",
    "                    # Check dataframe_1 for extra columns\n",
    "                    extra_columns_1 = [col for col in dataframe_1.columns if col not in expected_columns_dataframe_1]\n",
    "                    if extra_columns_1:\n",
    "                        # Removing extra columns from the DataFrame\n",
    "                        dataframe_1 = dataframe_1[expected_columns_dataframe_1]\n",
    "                    \n",
    "                    # Check dataframe_2 for extra columns\n",
    "                    extra_columns_2 = [col for col in dataframe_2.columns if col not in expected_columns_dataframe_2]\n",
    "                    if extra_columns_2:\n",
    "                        # Removing extra columns from the DataFrame\n",
    "                        dataframe_2 = dataframe_2[expected_columns_dataframe_2]\n",
    "                    \n",
    "                    # Dropping the redundant columns\n",
    "                    dataframe_1.drop(\n",
    "                        ['Weighted Average Price', \n",
    "                         'Weighted Average Yield', \n",
    "                         'ISIN No.', \n",
    "                         'Face Value'], \n",
    "                        axis=1, inplace=True)\n",
    "                    \n",
    "                    dataframe_2.drop(\n",
    "                        ['Scrip Code', \n",
    "                         'Open Price', \n",
    "                         'High Price', \n",
    "                         'Low Price', \n",
    "                         'No. of Trades', \n",
    "                         'Total Trade Volume (Face Value in Rs. Lakh)'], \n",
    "                        axis=1, inplace=True)\n",
    "                    \n",
    "                    # Adding columns to both the DataFrames\n",
    "                    date_to_fill = datetime.strptime(self.date, \"%d%m%Y\")\n",
    "                    dataframe_1['Date'] = date_to_fill\n",
    "                    dataframe_2['Date'] = date_to_fill\n",
    "                    \n",
    "                    # Renaming the columns\n",
    "                    dataframe_1.rename(\n",
    "                        columns={'Security Code': 'SecurityCode',\n",
    "                                 'Issuer Name': 'SecurityName',\n",
    "                                 'Coupon (%)': 'CouponRate',\n",
    "                                 'LTP': 'LastTradedPrice',\n",
    "                                 'Maturity Date': 'MaturityDate',\n",
    "                                 'Turnover (Rs. Lakh)': 'NetTurnover'},\n",
    "                        inplace=True)\n",
    "\n",
    "                    dataframe_2.rename(\n",
    "                        columns={'Security Code': 'SecurityCode',\n",
    "                                 'Security Description': 'SecurityName',\n",
    "                                 'Coupon Rate (%)': 'CouponRate',\n",
    "                                 'Close Price': 'LastTradedPrice',\n",
    "                                 'Total Trade Turnover (Rs. Lakh)': 'NetTurnover'},\n",
    "                        inplace=True)\n",
    "\n",
    "                    # Sort both the DataFrames to have columns in an alphabetically ascending order\n",
    "                    columns_in_ascending_order = sorted(dataframe_1.columns)\n",
    "                    dataframe_1 = dataframe_1[columns_in_ascending_order]\n",
    "                    \n",
    "                    columns_in_ascending_order = sorted(dataframe_2.columns)\n",
    "                    dataframe_2 = dataframe_2[columns_in_ascending_order]\n",
    "                    \n",
    "                    # Concatenate the two DataFrames\n",
    "                    concatenated_dataframe = pd.concat([dataframe_1, dataframe_2], axis=0, ignore_index=True)\n",
    "                    concatenated_dataframe['MaturityDate'] = pd.to_datetime(concatenated_dataframe['MaturityDate'], errors='coerce')\n",
    "                    \n",
    "                    return concatenated_dataframe\n",
    "                else:\n",
    "                    return pd.DataFrame()\n",
    "            else:\n",
    "                return pd.DataFrame()\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle exceptions here, or at least log them for debugging\n",
    "            print(f\"An error occurred while fetching the Bonds Data: {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def Bond_Price_Data(constructor_link, sql_connector, table_name, working_dates):\n",
    "        \"\"\"\n",
    "        Executes the bond data fetching and dumping process.\n",
    "\n",
    "        Args:\n",
    "            constructor_link (str): The base URL for constructing the download link.\n",
    "            sql_connector (SQL): An instance of the SQL class for database connection.\n",
    "            table_name (str): The name of the SQL table to dump data into.\n",
    "            working_dates (list): A list of dates for which data should be fetched and dumped.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            for date in working_dates:\n",
    "                # Initialize the Request_Bond_Data class instance\n",
    "                request = Request_Bond_Data(constructor_link, date)\n",
    "                dataframe = request.Fetching()\n",
    "                \n",
    "                # Performing the dumping only for dataframes with data\n",
    "                if not dataframe.empty:\n",
    "                    sql_connector.Append_SQL_Table(table_name=table_name, current_date=None, dataframe=dataframe)\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "            print(\"Executed Successfully!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle exceptions here, or at least log them for debugging\n",
    "            print(f\"An error occurred while dumping the Bond Price Data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24161b3",
   "metadata": {},
   "source": [
    "### Class: Request Equity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "403e2dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Request_Equity_Data:\n",
    "    def __init__(self, constructor_link, date):\n",
    "        \"\"\"\n",
    "        Initializes the Equity_Data class.\n",
    "\n",
    "        Args:\n",
    "            constructor_link (str): The base URL for constructing the download link.\n",
    "            date (str): The date for which data should be fetched.\n",
    "            sql_connector (SQL): An instance of the SQL class for database connection.\n",
    "            table_name (str): The name of the SQL table to dump data into.\n",
    "        \"\"\"\n",
    "        self.constructor_link = constructor_link\n",
    "        self.date = date\n",
    "        \n",
    "        self.url = constructor_link + date + '.zip'\n",
    "        self.csv_file_name = 'EQ_ISINCODE_' + date + '.CSV'\n",
    "        self.current_date = None\n",
    "        \n",
    "    def Fetching(self):\n",
    "        \"\"\"\n",
    "        Fetches and processes equity data.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the fetched data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            encoded_url = urllib.parse.quote(self.url, safe=':/')\n",
    "\n",
    "            # Send a GET request to download the zip file\n",
    "            response = requests.get(encoded_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                # Save the zip file locally\n",
    "                with open('downloaded_stock_price_data.zip', 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "\n",
    "                # Extract the contents of the zip file to a folder\n",
    "                with zipfile.ZipFile('downloaded_stock_price_data.zip', 'r') as zip_ref:\n",
    "                    zip_ref.extractall('extracted_stock_price_data')\n",
    "\n",
    "                # Specify the directory where the CSV file is located\n",
    "                folder_path = 'extracted_stock_price_data'\n",
    "\n",
    "                # Check if the CSV file exists in the folder\n",
    "                if self.csv_file_name in os.listdir(folder_path):\n",
    "                    # Create the full path to the CSV file\n",
    "                    csv_file_path = os.path.join(folder_path, self.csv_file_name)\n",
    "\n",
    "                    # Read the CSV file into a pandas DataFrame\n",
    "                    dataframe = pd.read_csv(csv_file_path, on_bad_lines='skip')\n",
    "                    \n",
    "                    expected_columns = [\n",
    "                        'SC_CODE',\n",
    "                        'SC_NAME',\n",
    "                        'SC_GROUP',\n",
    "                        'SC_TYPE',\n",
    "                        'OPEN',\n",
    "                        'HIGH',\n",
    "                        'LOW',\n",
    "                        'CLOSE',\n",
    "                        'LAST',\n",
    "                        'PREVCLOSE',\n",
    "                        'NO_TRADES',\n",
    "                        'NO_OF_SHRS',\n",
    "                        'NET_TURNOV',\n",
    "                        'TDCLOINDI',\n",
    "                        'ISIN_CODE',\n",
    "                        'TRADING_DATE',\n",
    "                        'FILLER2',\n",
    "                        'FILLER3'\n",
    "                    ]\n",
    "                    \n",
    "                    # Check DataFrame for extra columns\n",
    "                    extra_columns = [col for col in dataframe.columns if col not in expected_columns]\n",
    "                    if extra_columns:\n",
    "                        # Removing extra columns from the DataFrame\n",
    "                        dataframe = dataframe[expected_columns]\n",
    "                    \n",
    "                    # Renaming the columns\n",
    "                    dataframe.rename(\n",
    "                        columns={'SC_CODE': 'SCCode',\n",
    "                                 'SC_NAME': 'Ticker',\n",
    "                                 'OPEN': 'OpenPrice',\n",
    "                                 'HIGH': 'HighPrice',\n",
    "                                 'LOW': 'LowPrice',\n",
    "                                 'CLOSE': 'ClosePrice',\n",
    "                                 'LAST': 'LastPrice',\n",
    "                                 'PREVCLOSE': 'PreviousClose',\n",
    "                                 'NO_TRADES': 'NumberOfTrades',\n",
    "                                 'NO_OF_SHRS': 'NumberOfShares',\n",
    "                                 'NET_TURNOV': 'NetTurnover',\n",
    "                                 'ISIN_CODE': 'ISINCode',\n",
    "                                 'TRADING_DATE': 'Date'},\n",
    "                        inplace=True)\n",
    "                    \n",
    "                    # Define a list of columns that need data type conversion\n",
    "                    columns_to_convert_to_numeric = [\n",
    "                        'SCCode', \n",
    "                        'OpenPrice', \n",
    "                        'HighPrice', \n",
    "                        'LowPrice', \n",
    "                        'ClosePrice', \n",
    "                        'LastPrice', \n",
    "                        'PreviousClose', \n",
    "                        'NumberOfTrades', \n",
    "                        'NumberOfShares', \n",
    "                        'NetTurnover'\n",
    "                    ]\n",
    "                    \n",
    "                    dataframe[columns_to_convert_to_numeric] = dataframe[columns_to_convert_to_numeric].apply(pd.to_numeric, errors='coerce')\n",
    "                    \n",
    "                    # Dropping the redundant columns\n",
    "                    dataframe.drop(['SC_GROUP', 'SC_TYPE', 'TDCLOINDI', 'FILLER2', 'FILLER3'], axis=1, inplace=True)\n",
    "                    \n",
    "                    return dataframe\n",
    "                else:\n",
    "                    return pd.DataFrame()\n",
    "            else:\n",
    "                return pd.DataFrame()\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle exceptions here, or at least log them for debugging\n",
    "            print(f\"An error occurred while fetching the Stock Price data: {e}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def Stock_Price_Data(constructor_link, sql_connector, table_name, working_dates):\n",
    "        \"\"\"\n",
    "        Executes the equity data fetching and dumping process.\n",
    "\n",
    "        Args:\n",
    "            working_dates (list): A list of dates for which data should be fetched and dumped.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            for date in working_dates:\n",
    "                # Initialize the Request_Equity_Data class instance\n",
    "                request = Request_Equity_Data(constructor_link, date)\n",
    "                dataframe = request.Fetching()\n",
    "                \n",
    "                # Performing the dumping only for dataframes with data  \n",
    "                if not dataframe.empty:\n",
    "                    sql_connector.Append_SQL_Table(table_name=table_name, current_date=None, dataframe=dataframe)\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "            print(\"Executed Successfully!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle exceptions here, or at least log them for debugging\n",
    "            print(f\"An error occurred during the dumping of the Stock Price Data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80be4bee",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af3f6b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to adjust a date to the nearest weekday if it's a weekend\n",
    "def Adjust_To_Nearest_Weekday(date):\n",
    "    \"\"\"\n",
    "    Adjusts a given date to the nearest weekday if it falls on a weekend.\n",
    "\n",
    "    Args:\n",
    "        date (datetime.date): The date to be adjusted.\n",
    "\n",
    "    Returns:\n",
    "        datetime.date: The adjusted date.\n",
    "    \"\"\"\n",
    "    if date.weekday() == 5:  # Saturday\n",
    "        return date - timedelta(days=1)\n",
    "    elif date.weekday() == 6:  # Sunday\n",
    "        return date + timedelta(days=1)\n",
    "    else:\n",
    "        return date\n",
    "\n",
    "# Define a function to check if a date is a working day and not a holiday\n",
    "def Is_Working_Day(date):\n",
    "    \"\"\"\n",
    "    Checks if a date is a working day and not a holiday.\n",
    "\n",
    "    Args:\n",
    "        date (datetime.date): The date to be checked.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the date is a working day and not a holiday, False otherwise.\n",
    "    \"\"\"\n",
    "    return date.weekday() < 5 and date.strftime('%d%m%Y') not in Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "962c407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the start date (5 years back from today)\n",
    "start_date = date.today() - timedelta(days=5 * 365)\n",
    "\n",
    "# Define the end date (today's date)\n",
    "end_date = date.today()\n",
    "\n",
    "# Adjust start_date and end_date if they fall on weekends\n",
    "start_date = Adjust_To_Nearest_Weekday(start_date)\n",
    "end_date = Adjust_To_Nearest_Weekday(end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72c57105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the constructor link\n",
    "Bond_Constructor_Link = 'https://www.bseindia.com/download/Bhavcopy/Debt/DEBTBHAVCOPY'\n",
    "\n",
    "# List to store the working dates\n",
    "Bond_Working_Dates = []\n",
    "\n",
    "# List of holidays (in 'ddmmyy' format) for avoiding unnecessary requests\n",
    "Holidays = [\n",
    "    \"260118\", \"130218\", \"020318\", \"290318\", \"300318\", \"010518\", \"150818\", \"220818\",\n",
    "    \"130918\", \"200918\", \"021018\", \"181018\", \"071118\", \"081118\", \"231118\", \"251218\",\n",
    "    \"040319\", \"210319\", \"170419\", \"190419\", \"290419\", \"010519\", \"050619\", \"120819\",\n",
    "    \"150819\", \"020919\", \"100919\", \"021019\", \"081019\", \"211019\", \"281019\", \"121119\",\n",
    "    \"251219\", \"210220\", \"100320\", \"020420\", \"060420\", \"140420\", \"100420\", \"010520\",\n",
    "    \"250520\", \"021020\", \"161120\", \"301120\", \"251220\", \"260121\", \"110321\", \"290321\", \n",
    "    \"140421\", \"210421\", \"130521\", \"210721\", \"190821\", \"100921\", \"151021\", \"041121\", \n",
    "    \"051121\", \"191121\", \"260123\", \"070323\", \"300323\", \"040423\", \"070423\", \"140423\", \n",
    "    \"010523\", \"290623\", \"150823\", \"190923\", \"021023\", \"141123\", \"271123\", \"251223\"\n",
    "]\n",
    "\n",
    "# Generate working dates and add them to the list\n",
    "current_date = start_date\n",
    "while current_date < end_date:\n",
    "    if Is_Working_Day(current_date):\n",
    "        formatted_date = current_date.strftime('%d%m%Y')\n",
    "        Bond_Working_Dates.append(formatted_date)\n",
    "    current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc28ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the constructor link\n",
    "Equity_Constructor_Link = 'https://www.bseindia.com/download/BhavCopy/Equity/EQ_ISINCODE_'\n",
    "\n",
    "# List to store the working dates\n",
    "Equity_Working_Dates = []\n",
    "\n",
    "# List of holidays (in 'ddmmyy' format) for avoiding unnecessary requests\n",
    "Holidays = [\n",
    "    \"260118\", \"130218\", \"020318\", \"290318\", \"300318\", \"010518\", \"150818\", \"220818\",\n",
    "    \"130918\", \"200918\", \"021018\", \"181018\", \"071118\", \"081118\", \"231118\", \"251218\",\n",
    "    \"040319\", \"210319\", \"170419\", \"190419\", \"290419\", \"010519\", \"050619\", \"120819\",\n",
    "    \"150819\", \"020919\", \"100919\", \"021019\", \"081019\", \"211019\", \"281019\", \"121119\",\n",
    "    \"251219\", \"210220\", \"100320\", \"020420\", \"060420\", \"140420\", \"100420\", \"010520\",\n",
    "    \"250520\", \"021020\", \"161120\", \"301120\", \"251220\", \"260121\", \"110321\", \"290321\", \n",
    "    \"140421\", \"210421\", \"130521\", \"210721\", \"190821\", \"100921\", \"151021\", \"041121\", \n",
    "    \"051121\", \"191121\", \"260123\", \"070323\", \"300323\", \"040423\", \"070423\", \"140423\", \n",
    "    \"010523\", \"290623\", \"150823\", \"190923\", \"021023\", \"141123\", \"271123\", \"251223\"\n",
    "]\n",
    "\n",
    "# Generate working dates and add them to the list\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    if Is_Working_Day(current_date):\n",
    "        formatted_date = current_date.strftime('%d%m%y')\n",
    "        Equity_Working_Dates.append(formatted_date)\n",
    "    current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac290d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed Successfully!\n",
      "Executed Successfully!\n",
      "\n",
      "Time taken to dump the HTTP Requested Files:  1.0  Hours\n"
     ]
    }
   ],
   "source": [
    "# Defining the table name\n",
    "Stock_Price_Data_Table_Name = \"StockPriceData\"\n",
    "Bond_Price_Data_Table_Name = \"BondPriceData\"\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "# Execute the data dumping\n",
    "Request_Bond_Data.Bond_Price_Data(Bond_Constructor_Link, SQL_Connector, Bond_Price_Data_Table_Name, Bond_Working_Dates)\n",
    "Request_Equity_Data.Stock_Price_Data(Equity_Constructor_Link, SQL_Connector, Stock_Price_Data_Table_Name, Equity_Working_Dates)\n",
    "\n",
    "t_end = time.time()\n",
    "\n",
    "print(\"\\nTime taken to dump the HTTP Requested Files: \", round((t_end - t_start) / 3600, 0), \" Hours\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
